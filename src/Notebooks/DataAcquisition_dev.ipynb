{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Avro Blobs Into Parquet Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Importing and Environment Variable Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, SQLContext, Window\n",
    "from pyspark.sql.functions import udf, mean, lit, stddev, col, expr, when, date_sub, avg, window\n",
    "from pyspark.sql.types import DoubleType, ArrayType, ShortType, LongType, IntegerType, TimestampType, StructType, StringType, StructField\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema\n",
    "from azure.storage.table import TableService, Entity, TablePermissions\n",
    "\n",
    "STORAGE_ACCOUNT_SUFFIX = 'core.windows.net'\n",
    "TELEMETRY_STORAGE_ACCOUNT_NAME = os.getenv('TELEMETRY_STORAGE_ACCOUNT_NAME')\n",
    "TELEMETRY_STORAGE_ACCOUNT_KEY = os.getenv('TELEMETRY_STORAGE_ACCOUNT_KEY')\n",
    "TABLE_STORAGE_ACCOUNT_NAME = os.getenv('TABLE_STORAGE_ACCOUNT_NAME') #TODO need to add to ENV Variables\n",
    "TABLE_STORAGE_ACCOUNT_KEY = os.getenv('TABLE_STORAGE_ACCOUNT_KEY') #TODO need to add to ENV Variables\n",
    "TELEMETRY_CONTAINER_NAME = os.getenv('TELEMETRY_CONTAINER_NAME')\n",
    "#LOGS_ARCHIVE_CONTAINER_NAME = 'logs-archive' #TODO: introduce environment variables\n",
    "STAGING_STORAGE_ACCOUNT_NAME = os.getenv('STAGING_STORAGE_ACCOUNT_NAME')\n",
    "STAGING_STORAGE_ACCOUNT_KEY = os.getenv('STAGING_STORAGE_ACCOUNT_KEY')\n",
    "LOG_TABLE_NAME = os.getenv('LOG_TABLE_NAME') #TODO need to add this to ENV Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Development Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For development purposes only until ENV Variables get set\n",
    "STORAGE_ACCOUNT_SUFFIX = 'core.windows.net'\n",
    "TELEMETRY_STORAGE_ACCOUNT_NAME = \"stguwsjc4vojbrbw\"\n",
    "TELEMETRY_STORAGE_ACCOUNT_KEY = \"2GGDWVBtGBy+hAgl5a1uGT4NeU2zzFdocuDFKnOwR2vc5wEOP7jTfbS3/Nl5vvzEudJ4nfH6ozmSOSPXo3xETA==\"\n",
    "TABLE_STORAGE_ACCOUNT_NAME = \"stgmosxhnn5t3nyc\"\n",
    "TABLE_STORAGE_ACCOUNT_KEY = \"fa0TlK+KYvXfLXGqfqVATbNF4xB5o64O4i7PjmlGMSTXCLMIOh/9Sc4ScpJ2V0vUzQ2TlK4wVu0BbA9i3HZGaw==\"\n",
    "TELEMETRY_CONTAINER_NAME = \"telemetry\"\n",
    "LOG_TABLE_NAME = 'logs'\n",
    "LOGS_ARCHIVE_CONTAINER_NAME = \"logs\" #TODO: introduce environment variables\n",
    "STAGING_STORAGE_ACCOUNT_NAME = \"stguwsjc4vojbrbw\"\n",
    "STAGING_STORAGE_ACCOUNT_KEY = \"2GGDWVBtGBy+hAgl5a1uGT4NeU2zzFdocuDFKnOwR2vc5wEOP7jTfbS3/Nl5vvzEudJ4nfH6ozmSOSPXo3xETA==\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Drop Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_dir = str(Path.home()) + '/data'\n",
    "\n",
    "#TODO: Convert data_dir into env variable\n",
    "% rm -rf $data_dir\n",
    "% mkdir $data_dir $data_dir/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving telemetry data (as spark dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+\n",
      "|     EnqueuedTimeUtc|Properties|    SystemProperties|                Body|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wasbTelemetryUrl = \"wasb://{0}@{1}.blob.{2}/*/*/*/*/*/*/*\".format(TELEMETRY_CONTAINER_NAME, \n",
    "                                                                  TELEMETRY_STORAGE_ACCOUNT_NAME, \n",
    "                                                                  STORAGE_ACCOUNT_SUFFIX)\n",
    "\n",
    "sc = SparkSession.builder.getOrCreate()\n",
    "hc = sc._jsc.hadoopConfiguration()\n",
    "hc.set(\"avro.mapred.ignore.inputs.without.extension\", \"false\")\n",
    "if TELEMETRY_STORAGE_ACCOUNT_KEY:\n",
    "     hc.set(\"fs.azure.account.key.{}.blob.core.windows.net\".format(TELEMETRY_STORAGE_ACCOUNT_NAME), TELEMETRY_STORAGE_ACCOUNT_KEY)\n",
    "hc.set(\"fs.azure.account.key.{}.blob.core.windows.net\"\n",
    "    .format(STAGING_STORAGE_ACCOUNT_NAME), STAGING_STORAGE_ACCOUNT_KEY)\n",
    "sql = SQLContext.getOrCreate(sc)\n",
    "avroblob = sql.read.format(\"com.databricks.spark.avro\").load(wasbTelemetryUrl)\n",
    "avroblob.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert byteformatted \"body\" of raw blob data into JSON, then explode result into new Pyspark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EnqueuedTimeUtc: string (nullable = true)\n",
      " |-- Properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- SystemProperties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- Body: binary (nullable = true)\n",
      " |-- BodyString: string (nullable = true)\n",
      "\n",
      "+----------------+-------------------+-----------+--------+-------+-------------+-----------+--------------------+---------+\n",
      "|ambient_pressure|ambient_temperature|  machineID|pressure|  speed|speed_desired|temperature|           timestamp|vibration|\n",
      "+----------------+-------------------+-----------+--------+-------+-------------+-----------+--------------------+---------+\n",
      "|          100.92|              20.06|Machine-004|  655.58|1133.27|         1000|     174.62|2018-06-20T16:41:...|     null|\n",
      "|          101.02|              19.95|Machine-001|   46.46|  -2.17|            0|     164.84|2018-06-20T16:41:...|     null|\n",
      "|          101.04|              20.06|Machine-004|  673.66|1131.69|         1000|     174.71|2018-06-20T16:41:...|     null|\n",
      "|          101.07|              20.05|Machine-001|   70.93|   1.97|            0|     164.95|2018-06-20T16:41:...|     null|\n",
      "|           101.1|               19.9|Machine-004|  660.65|1138.23|         1000|     174.63|2018-06-20T16:41:...|     null|\n",
      "|          100.94|              19.94|Machine-001|  211.62| 560.03|         1000|     138.58|2018-06-20T16:41:...|     null|\n",
      "|          101.05|              20.06|Machine-001|  482.56| 829.28|         1000|     138.49|2018-06-20T16:41:...|     null|\n",
      "|          100.93|              20.07|Machine-001|  768.96| 967.38|         1000|     138.42|2018-06-20T16:41:...|     null|\n",
      "|          100.97|              20.05|Machine-001| 1014.26|1038.19|         1000|     138.43|2018-06-20T16:41:...|     null|\n",
      "|          101.03|              20.09|Machine-001| 1176.04|1074.96|         1000|     138.61|2018-06-20T16:41:...|     null|\n",
      "|          100.92|              19.93|Machine-004|  668.61|1137.07|         1000|     174.73|2018-06-20T16:41:...|     null|\n",
      "|          100.91|              20.04|Machine-004|  660.71|1136.41|         1000|     174.74|2018-06-20T16:41:...|     null|\n",
      "|          100.95|              19.91|Machine-001|   52.92|  -2.84|            0|     164.96|2018-06-20T16:41:...|     null|\n",
      "|          101.05|              20.09|Machine-001|   68.23|   2.02|            0|     164.94|2018-06-20T16:41:...|     null|\n",
      "|          101.07|              20.02|Machine-001|   45.18|   1.46|            0|     164.92|2018-06-20T16:41:...|     null|\n",
      "|          101.04|              20.06|Machine-001|   56.86|    4.3|            0|     165.02|2018-06-20T16:41:...|     null|\n",
      "|          100.95|              19.92|Machine-004|  639.36|1131.49|         1000|     174.83|2018-06-20T16:41:...|     null|\n",
      "|          100.97|              19.96|Machine-001|   45.55|   2.79|            0|      164.9|2018-06-20T16:41:...|     null|\n",
      "|          101.04|              20.03|Machine-004|  664.96| 1136.1|         1000|     174.74|2018-06-20T16:41:...|     null|\n",
      "|          101.04|               20.0|Machine-001|   77.71|   4.97|            0|      165.0|2018-06-20T16:41:...|     null|\n",
      "+----------------+-------------------+-----------+--------+-------+-------------+-----------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert byteformat to string format in pyspark dataframe\n",
    "from json import loads as Loads\n",
    "column = avroblob['Body']\n",
    "string_udf = udf(lambda x: x.decode(\"utf-8\"))\n",
    "avroblob=avroblob.withColumn(\"BodyString\", string_udf(column))\n",
    "avroblob.printSchema()\n",
    "\n",
    "#Convert \"body\" into new DataFrame\n",
    "telemetry_df = sql.read.json(avroblob.select(\"BodyString\").rdd.map(lambda r: r.BodyString))\n",
    "telemetry_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns to retain: timestamp, ambient_pressure, ambient_temperature machineID, pressure, speed, \n",
    "#                   speed_desired, temperature\n",
    "subsetted_df = telemetry_df.select([\"timestamp\", \"ambient_pressure\",\"ambient_temperature\",\"machineID\",\"pressure\",\"speed\",\"speed_desired\",\"temperature\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- ambient_pressure: double (nullable = true)\n",
      " |-- ambient_temperature: double (nullable = true)\n",
      " |-- machineID: string (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- speed_desired: long (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#modify timestamp format\n",
    "import datetime\n",
    "e = '%Y-%m-%dT%H:%M:%S.%f'\n",
    "timestamp_udf = udf(lambda date: datetime.datetime.strptime(date, e), TimestampType())\n",
    "reformatted_time_df = subsetted_df.withColumn(\"timestamp\", timestamp_udf(subsetted_df[\"timestamp\"]))\n",
    "\n",
    "reformatted_time_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataframe to Parquet in system storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_time_df.write.parquet(data_dir+\"/telemetry\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table retrieval\n",
    "table_service = TableService(account_name=TABLE_STORAGE_ACCOUNT_NAME, account_key=TABLE_STORAGE_ACCOUNT_KEY)\n",
    "tblob = table_service.query_entities(LOG_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process log table data into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>PartitionKey</th>\n",
       "      <th>Message</th>\n",
       "      <th>Code</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>etag</th>\n",
       "      <th>RowKey</th>\n",
       "      <th>_Driver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEBUG</td>\n",
       "      <td>Machine-000</td>\n",
       "      <td>{\"h1\": 0.9374248680712486, \"h2\": 0.92468197881...</td>\n",
       "      <td>SIM_HEALTH</td>\n",
       "      <td>2018-06-25 13:21:01.766912</td>\n",
       "      <td>W/\"datetime'2018-06-25T13%3A21%3A01.7669126Z'\"</td>\n",
       "      <td>008e47d2214d4d29a1cecec93bc8d6ef</td>\n",
       "      <td>46132c63-8f3a-4fc7-acf8-50a5d8fa94ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEBUG</td>\n",
       "      <td>Machine-000</td>\n",
       "      <td>{\"h1\": 0.9376092354171048, \"h2\": 0.92541503564...</td>\n",
       "      <td>SIM_HEALTH</td>\n",
       "      <td>2018-06-25 09:15:49.483992</td>\n",
       "      <td>W/\"datetime'2018-06-25T09%3A15%3A49.483992Z'\"</td>\n",
       "      <td>024fb3d61ebf45a28d369fa0680b7941</td>\n",
       "      <td>46132c63-8f3a-4fc7-acf8-50a5d8fa94ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEBUG</td>\n",
       "      <td>Machine-000</td>\n",
       "      <td>{\"h1\": 0.9372548374119425, \"h2\": 0.92398388622...</td>\n",
       "      <td>SIM_HEALTH</td>\n",
       "      <td>2018-06-25 17:30:44.172884</td>\n",
       "      <td>W/\"datetime'2018-06-25T17%3A30%3A44.1728843Z'\"</td>\n",
       "      <td>0259e7d02bc04c65aeec4d5bbd4fcd9f</td>\n",
       "      <td>46132c63-8f3a-4fc7-acf8-50a5d8fa94ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEBUG</td>\n",
       "      <td>Machine-000</td>\n",
       "      <td>{\"h1\": 0.9375956117653576, \"h2\": 0.92536168736...</td>\n",
       "      <td>SIM_HEALTH</td>\n",
       "      <td>2018-06-25 09:33:56.598835</td>\n",
       "      <td>W/\"datetime'2018-06-25T09%3A33%3A56.5988351Z'\"</td>\n",
       "      <td>079f69861a314898b76f540ca0fe9fe8</td>\n",
       "      <td>46132c63-8f3a-4fc7-acf8-50a5d8fa94ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEBUG</td>\n",
       "      <td>Machine-000</td>\n",
       "      <td>{\"h1\": 0.9373586393360502, \"h2\": 0.92441264690...</td>\n",
       "      <td>SIM_HEALTH</td>\n",
       "      <td>2018-06-25 14:48:46.091508</td>\n",
       "      <td>W/\"datetime'2018-06-25T14%3A48%3A46.0915087Z'\"</td>\n",
       "      <td>08880c1fa6eb4844a161aab941a821a9</td>\n",
       "      <td>46132c63-8f3a-4fc7-acf8-50a5d8fa94ae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Level PartitionKey                                            Message  \\\n",
       "0  DEBUG  Machine-000  {\"h1\": 0.9374248680712486, \"h2\": 0.92468197881...   \n",
       "1  DEBUG  Machine-000  {\"h1\": 0.9376092354171048, \"h2\": 0.92541503564...   \n",
       "2  DEBUG  Machine-000  {\"h1\": 0.9372548374119425, \"h2\": 0.92398388622...   \n",
       "3  DEBUG  Machine-000  {\"h1\": 0.9375956117653576, \"h2\": 0.92536168736...   \n",
       "4  DEBUG  Machine-000  {\"h1\": 0.9373586393360502, \"h2\": 0.92441264690...   \n",
       "\n",
       "         Code                  Timestamp  \\\n",
       "0  SIM_HEALTH 2018-06-25 13:21:01.766912   \n",
       "1  SIM_HEALTH 2018-06-25 09:15:49.483992   \n",
       "2  SIM_HEALTH 2018-06-25 17:30:44.172884   \n",
       "3  SIM_HEALTH 2018-06-25 09:33:56.598835   \n",
       "4  SIM_HEALTH 2018-06-25 14:48:46.091508   \n",
       "\n",
       "                                             etag  \\\n",
       "0  W/\"datetime'2018-06-25T13%3A21%3A01.7669126Z'\"   \n",
       "1   W/\"datetime'2018-06-25T09%3A15%3A49.483992Z'\"   \n",
       "2  W/\"datetime'2018-06-25T17%3A30%3A44.1728843Z'\"   \n",
       "3  W/\"datetime'2018-06-25T09%3A33%3A56.5988351Z'\"   \n",
       "4  W/\"datetime'2018-06-25T14%3A48%3A46.0915087Z'\"   \n",
       "\n",
       "                             RowKey                               _Driver  \n",
       "0  008e47d2214d4d29a1cecec93bc8d6ef  46132c63-8f3a-4fc7-acf8-50a5d8fa94ae  \n",
       "1  024fb3d61ebf45a28d369fa0680b7941  46132c63-8f3a-4fc7-acf8-50a5d8fa94ae  \n",
       "2  0259e7d02bc04c65aeec4d5bbd4fcd9f  46132c63-8f3a-4fc7-acf8-50a5d8fa94ae  \n",
       "3  079f69861a314898b76f540ca0fe9fe8  46132c63-8f3a-4fc7-acf8-50a5d8fa94ae  \n",
       "4  08880c1fa6eb4844a161aab941a821a9  46132c63-8f3a-4fc7-acf8-50a5d8fa94ae  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = list()\n",
    "for row in tblob:\n",
    "    if (len(attributes) == 0):\n",
    "        for attribute in row:\n",
    "            attributes.append(attribute)\n",
    "    break\n",
    "log_df = pd.DataFrame(columns=attributes)\n",
    "for row in tblob:\n",
    "    row_dict = {}    \n",
    "    for attribute in row:\n",
    "        if (attribute != \"Timestamp\"):\n",
    "            row_dict[attribute] = row[attribute]\n",
    "        else:\n",
    "            newtime = row[attribute].replace(tzinfo=None)\n",
    "            timeitem = pd.Timestamp(newtime, tz=None)\n",
    "            row_dict[attribute] = timeitem\n",
    "    log_df = log_df.append(row_dict, ignore_index=True)\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select necessary attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>level</th>\n",
       "      <th>machineID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-06-25 13:21:01.766912</th>\n",
       "      <td>SIM_HEALTH</td>\n",
       "      <td>DEBUG</td>\n",
       "      <td>Machine-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25 09:15:49.483992</th>\n",
       "      <td>SIM_HEALTH</td>\n",
       "      <td>DEBUG</td>\n",
       "      <td>Machine-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25 17:30:44.172884</th>\n",
       "      <td>SIM_HEALTH</td>\n",
       "      <td>DEBUG</td>\n",
       "      <td>Machine-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25 09:33:56.598835</th>\n",
       "      <td>SIM_HEALTH</td>\n",
       "      <td>DEBUG</td>\n",
       "      <td>Machine-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25 14:48:46.091508</th>\n",
       "      <td>SIM_HEALTH</td>\n",
       "      <td>DEBUG</td>\n",
       "      <td>Machine-000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  code  level    machineID\n",
       "timestamp                                                 \n",
       "2018-06-25 13:21:01.766912  SIM_HEALTH  DEBUG  Machine-000\n",
       "2018-06-25 09:15:49.483992  SIM_HEALTH  DEBUG  Machine-000\n",
       "2018-06-25 17:30:44.172884  SIM_HEALTH  DEBUG  Machine-000\n",
       "2018-06-25 09:33:56.598835  SIM_HEALTH  DEBUG  Machine-000\n",
       "2018-06-25 14:48:46.091508  SIM_HEALTH  DEBUG  Machine-000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df = log_df[[\"Timestamp\", \"Code\", \"Level\", \"PartitionKey\"]]\n",
    "log_df.columns = [\"timestamp\", \"code\",\"level\",\"machineID\"]\n",
    "log_df.index = log_df['timestamp']\n",
    "del log_df['timestamp']\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write logs to system storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.to_parquet(data_dir+\"/logs/logs.parquet\", engine='fastparquet', times='int96')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
