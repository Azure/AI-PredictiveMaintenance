{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Avro Blobs Into Parquet Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, SQLContext, Window\n",
    "from pyspark.sql.functions import udf, mean, lit, stddev, col, expr, when, date_sub, avg, window\n",
    "from pyspark.sql.types import DoubleType, ArrayType, ShortType, LongType, IntegerType, TimestampType, StructType, StringType, StructField\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema\n",
    "\n",
    "STORAGE_ACCOUNT_SUFFIX = 'core.windows.net'\n",
    "TELEMETRY_STORAGE_ACCOUNT_NAME = \"stguwsjc4vojbrbw\"\n",
    "TELEMETRY_STORAGE_ACCOUNT_KEY = \"2GGDWVBtGBy+hAgl5a1uGT4NeU2zzFdocuDFKnOwR2vc5wEOP7jTfbS3/Nl5vvzEudJ4nfH6ozmSOSPXo3xETA==\"\n",
    "TELEMETRY_CONTAINER_NAME = \"telemetry\"\n",
    "LOGS_ARCHIVE_CONTAINER_NAME = 'logs-archive'\n",
    "STAGING_STORAGE_ACCOUNT_NAME = \"stguwsjc4vojbrbw\"\n",
    "STAGING_STORAGE_ACCOUNT_KEY = \"2GGDWVBtGBy+hAgl5a1uGT4NeU2zzFdocuDFKnOwR2vc5wEOP7jTfbS3/Nl5vvzEudJ4nfH6ozmSOSPXo3xETA==\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_dir = str(Path.home()) + '/data'\n",
    "\n",
    "% rm -rf $data_dir\n",
    "% mkdir $data_dir $data_dir/logs $data_dir/telemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+\n",
      "|     EnqueuedTimeUtc|Properties|    SystemProperties|                Body|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-20T16:41:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wasbTelemetryUrl = \"wasb://{0}@{1}.blob.{2}/*/*/*/*/*/*/*\".format(TELEMETRY_CONTAINER_NAME, \n",
    "                                                                  TELEMETRY_STORAGE_ACCOUNT_NAME, \n",
    "                                                                  STORAGE_ACCOUNT_SUFFIX)\n",
    "sc = SparkSession.builder.getOrCreate()\n",
    "hc = sc._jsc.hadoopConfiguration()\n",
    "hc.set(\"avro.mapred.ignore.inputs.without.extension\", \"false\")\n",
    "if TELEMETRY_STORAGE_ACCOUNT_KEY:\n",
    "     hc.set(\"fs.azure.account.key.{}.blob.core.windows.net\".format(TELEMETRY_STORAGE_ACCOUNT_NAME), TELEMETRY_STORAGE_ACCOUNT_KEY)\n",
    "hc.set(\"fs.azure.account.key.{}.blob.core.windows.net\"\n",
    "    .format(STAGING_STORAGE_ACCOUNT_NAME), STAGING_STORAGE_ACCOUNT_KEY)\n",
    "sql = SQLContext.getOrCreate(sc)\n",
    "avroblob = sql.read.format(\"com.databricks.spark.avro\").load(wasbTelemetryUrl)\n",
    "avroblob.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EnqueuedTimeUtc: string (nullable = true)\n",
      " |-- Properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- SystemProperties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- Body: binary (nullable = true)\n",
      " |-- BodyString: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert byteformat to string format in pyspark dataframe\n",
    "from json import loads as Loads\n",
    "column = avroblob['Body']\n",
    "string_udf = udf(lambda x: x.decode(\"utf-8\"))\n",
    "avroblob=avroblob.withColumn(\"BodyString\", string_udf(column))\n",
    "avroblob.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(EnqueuedTimeUtc='2018-06-20T16:41:39.0980000Z', Properties={}, SystemProperties={'connectionDeviceGenerationId': '636650280485038545', 'connectionDeviceId': 'Machine-004', 'connectionAuthMethod': '{\"scope\":\"device\",\"type\":\"sas\",\"issuer\":\"iothub\",\"acceptingIpFilterRule\":null}', 'enqueuedTime': '2018-06-20T16:41:39.0980000Z'}, Body=bytearray(b'{\"machineID\": \"Machine-004\", \"timestamp\": \"2018-06-20T16:41:39.086812\", \"speed_desired\": 1000, \"ambient_temperature\": 20.06, \"ambient_pressure\": 100.92, \"speed\": 1133.27, \"temperature\": 174.62, \"pressure\": 655.58, \"vibration\": null}'), BodyString='{\"machineID\": \"Machine-004\", \"timestamp\": \"2018-06-20T16:41:39.086812\", \"speed_desired\": 1000, \"ambient_temperature\": 20.06, \"ambient_pressure\": 100.92, \"speed\": 1133.27, \"temperature\": 174.62, \"pressure\": 655.58, \"vibration\": null}', BodyJson='{speed_desired=1000, ambient_temperature=20.06, machineID=Machine-004, ambient_pressure=100.92, temperature=174.62, pressure=655.58, vibration=null, speed=1133.27, timestamp=2018-06-20T16:41:39.086812}')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_udf = udf(lambda x: Loads(x))\n",
    "avroblob=avroblob.withColumn(\"BodyJson\", json_udf(avroblob[\"BodyString\"]))\n",
    "avroblob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = [\n",
    "    (\"ambient_temperature\", DoubleType()),\n",
    "    (\"ambient_pressure\", DoubleType()),\n",
    "    (\"speed\", DoubleType()),\n",
    "    (\"temperature\", DoubleType()),\n",
    "    (\"pressure\", DoubleType()),\n",
    "    (\"vibration\", ArrayType(ShortType()))]\n",
    "\n",
    "def extract_value(c):\n",
    "    \"\"\"Returns a PySpark UDF that un-pickles\n",
    "    the payload, then extracts a value by its key\n",
    "    performing a type conversion if needed.\n",
    "    \"\"\"\n",
    "    def f(x):\n",
    "        v = pickle.loads(x)[c[0]]\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = v.tolist()\n",
    "        if isinstance(c[1], DoubleType):            \n",
    "            v = float(v)\n",
    "        return v\n",
    "    return udf(f, c[1])\n",
    "\n",
    "telemetry_df = (telemetry_df\n",
    "      .withColumn('machineID', telemetry_df.SystemProperties['connectionDeviceId'])\n",
    "      .withColumn(\"timestamp\", telemetry_df['EnqueuedTimeUtc'].cast(TimestampType())))\n",
    "\n",
    "# Note: The reduce function provides a concise way of adding many columns to the dataframe at once.\n",
    "# We will be using this pattern a couple of times throughout this notebook.\n",
    "telemetry_df = (reduce(lambda _df, ic: _df.withColumn(ic[0], extract_value(ic)(telemetry_df.Body)), input_columns, telemetry_df)\n",
    "      .drop('Properties', 'SystemProperties', 'Body', 'EnqueuedTimeUtc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-6fa80839a314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtelemetry_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0}/telemetry/telemetry.parquet'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3614\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "telemetry_df.write.parquet('{0}/telemetry/telemetry.parquet'.format(data_dir), mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
