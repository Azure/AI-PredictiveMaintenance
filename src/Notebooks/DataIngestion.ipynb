{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Avro Blobs Into Parquet Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Importing and Environment Variable Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType, StringType\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from azure.storage.table import TableService"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For development purposes only until ENV Variables get set\n",
    "from pathlib import Path\n",
    "env_config_file_location = (str(Path.home())+\"/NotebookEnvironmentVariablesConfig.json\")\n",
    "config_file = Path(env_config_file_location)\n",
    "if not config_file.is_file():\n",
    "  env_config_file_location = (\"/dbfs\"+str(Path.home())+\"/NotebookEnvironmentVariablesConfig.json\")\n",
    "f = open(env_config_file_location)\n",
    "env_variables = json.load(f)[\"DataIngestion\"]\n",
    "\n",
    "STORAGE_ACCOUNT_SUFFIX = 'core.windows.net'\n",
    "STORAGE_ACCOUNT_NAME = env_variables[\"STORAGE_ACCOUNT_NAME\"]\n",
    "STORAGE_ACCOUNT_KEY = env_variables[\"STORAGE_ACCOUNT_KEY\"]\n",
    "TELEMETRY_CONTAINER_NAME = env_variables[\"TELEMETRY_CONTAINER_NAME\"]\n",
    "LOG_TABLE_NAME = env_variables[\"LOG_TABLE_NAME\"]\n",
    "DATA_ROOT = env_variables[\"DATA_ROOT_FOLDER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Drop Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = DATA_ROOT + '/data'\n",
    "\n",
    "#TODO: Convert data_dir into env variable\n",
    "% rm -rf $data_dir\n",
    "% mkdir $data_dir $data_dir/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving telemetry data (as spark dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+\n",
      "|     EnqueuedTimeUtc|Properties|    SystemProperties|                Body|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-28T19:51:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wasbTelemetryUrl = \"wasb://{0}@{1}.blob.{2}/*/*/*/*/*/*/*\".format(TELEMETRY_CONTAINER_NAME, \n",
    "                                                                  STORAGE_ACCOUNT_NAME, \n",
    "                                                                  STORAGE_ACCOUNT_SUFFIX)\n",
    "\n",
    "sc = SparkSession.builder.getOrCreate()\n",
    "hc = sc._jsc.hadoopConfiguration()\n",
    "hc.set(\"avro.mapred.ignore.inputs.without.extension\", \"false\")\n",
    "if STORAGE_ACCOUNT_KEY:\n",
    "     hc.set(\"fs.azure.account.key.{}.blob.core.windows.net\".format(STORAGE_ACCOUNT_NAME), STORAGE_ACCOUNT_KEY)\n",
    "hc.set(\"fs.azure.account.key.{}.blob.core.windows.net\"\n",
    "    .format(STORAGE_ACCOUNT_NAME), STORAGE_ACCOUNT_KEY)\n",
    "sql = SQLContext.getOrCreate(sc)\n",
    "avroblob = sql.read.format(\"com.databricks.spark.avro\").load(wasbTelemetryUrl)\n",
    "avroblob.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert byteformatted \"body\" of raw blob data into JSON, then explode result into new Pyspark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EnqueuedTimeUtc: string (nullable = true)\n",
      " |-- Properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- SystemProperties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- Body: binary (nullable = true)\n",
      " |-- BodyString: string (nullable = true)\n",
      "\n",
      "+----------------+-------------------+-----------+--------+-------+-------------+-----------+--------------------+---------+\n",
      "|ambient_pressure|ambient_temperature|  machineID|pressure|  speed|speed_desired|temperature|           timestamp|vibration|\n",
      "+----------------+-------------------+-----------+--------+-------+-------------+-----------+--------------------+---------+\n",
      "|          101.09|              19.92|machine-009|  988.07|1185.79|         1000|      159.3|2018-06-28T19:51:...|     null|\n",
      "|          100.91|              19.94|Machine-003| 1792.65|1180.34|         1000|     131.88|2018-06-28T19:51:...|     null|\n",
      "|          100.94|              19.91|Machine-001|  553.73|   4.75|            0|     128.92|2018-06-28T19:50:...|     null|\n",
      "|          100.94|               20.1|Machine-001| 1871.61|1150.64|         1000|      129.0|2018-06-28T19:51:...|     null|\n",
      "|          101.06|              19.95|Machine-001|  589.45|   4.91|            0|     128.87|2018-06-28T19:48:...|     null|\n",
      "|          100.93|              19.92|Machine-004| 1365.66|1108.38|         1000|     139.68|2018-06-28T19:51:...|     null|\n",
      "|          101.09|               20.0|machine-008|   787.6|1601.39|         1000|     192.09|2018-06-28T19:51:...|     null|\n",
      "|          101.03|              19.93|Machine-001| 1874.89|1156.37|         1000|     128.86|2018-06-28T19:47:...|     null|\n",
      "|          100.93|              19.97|machine-009|  992.38|1180.46|         1000|     159.15|2018-06-28T19:51:...|     null|\n",
      "|          100.92|              19.92|Machine-003| 1783.61|1178.14|         1000|     131.97|2018-06-28T19:51:...|     null|\n",
      "|          101.09|              20.04|Machine-001|  535.48|   0.24|            0|     129.04|2018-06-28T19:50:...|     null|\n",
      "|          101.05|              20.04|Machine-001| 1853.56|1149.76|         1000|     128.91|2018-06-28T19:48:...|     null|\n",
      "|          101.09|               20.0|Machine-004| 1402.24| 1103.7|         1000|     139.82|2018-06-28T19:51:...|     null|\n",
      "|          101.07|              20.05|machine-008|  770.06|1597.25|         1000|     192.46|2018-06-28T19:51:...|     null|\n",
      "|          100.95|              20.07|Machine-001| 1859.88|1156.81|         1000|     129.01|2018-06-28T19:50:...|     null|\n",
      "|          100.93|              19.92|machine-009|  997.37|1183.01|         1000|     159.29|2018-06-28T19:51:...|     null|\n",
      "|          100.95|              19.94|Machine-003| 1798.42|1176.51|         1000|     131.92|2018-06-28T19:51:...|     null|\n",
      "|          101.05|              20.07|Machine-001|  484.01|   2.58|            0|     129.04|2018-06-28T19:50:...|     null|\n",
      "|          100.91|              19.92|Machine-001| 1856.02|1156.78|         1000|     128.96|2018-06-28T19:51:...|     null|\n",
      "|          100.98|              19.98|Machine-004| 1369.28|1102.04|         1000|     139.68|2018-06-28T19:51:...|     null|\n",
      "+----------------+-------------------+-----------+--------+-------+-------------+-----------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert byteformat to string format in pyspark dataframe\n",
    "from json import loads as Loads\n",
    "column = avroblob['Body']\n",
    "string_udf = udf(lambda x: x.decode(\"utf-8\"))\n",
    "avroblob=avroblob.withColumn(\"BodyString\", string_udf(column))\n",
    "avroblob.printSchema()\n",
    "\n",
    "#Convert \"body\" into new DataFrame\n",
    "telemetry_df = sql.read.json(avroblob.select(\"BodyString\").rdd.map(lambda r: r.BodyString))\n",
    "telemetry_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns to retain: timestamp, ambient_pressure, ambient_temperature machineID, pressure, speed, \n",
    "#                   speed_desired, temperature\n",
    "subsetted_df = telemetry_df.select([\"timestamp\", \"ambient_pressure\",\"ambient_temperature\",\"machineID\",\"pressure\",\"speed\",\"speed_desired\",\"temperature\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- ambient_pressure: double (nullable = true)\n",
      " |-- ambient_temperature: double (nullable = true)\n",
      " |-- machineID: string (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- speed_desired: long (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "e = '%Y-%m-%dT%H:%M:%S.%f'\n",
    "reformatted_time_df = subsetted_df.withColumn(\"timestamp\", F.col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "reformatted_time_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataframe to Parquet in system storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_time_df.write.parquet(data_dir+\"/telemetry\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table retrieval\n",
    "table_service = TableService(account_name=STORAGE_ACCOUNT_NAME, account_key=STORAGE_ACCOUNT_KEY)\n",
    "tblob = table_service.query_entities(LOG_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process log table data into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>_Driver</th>\n",
       "      <th>Code</th>\n",
       "      <th>etag</th>\n",
       "      <th>Message</th>\n",
       "      <th>PartitionKey</th>\n",
       "      <th>RowKey</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFO</td>\n",
       "      <td>8621bba7-55e2-485f-b191-c7365d9f5847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W/\"datetime'2018-06-28T18%3A09%3A07.0277527Z'\"</td>\n",
       "      <td>Simulation started.</td>\n",
       "      <td>Machine-000</td>\n",
       "      <td>43a1504005d2467694a8f7b8b73888fb</td>\n",
       "      <td>2018-06-28 18:09:07.027752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFO</td>\n",
       "      <td>c465fc84-4710-4ac3-ae77-5648037b3cfe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W/\"datetime'2018-06-28T20%3A41%3A23.2824814Z'\"</td>\n",
       "      <td>Simulation started.</td>\n",
       "      <td>Machine-001</td>\n",
       "      <td>105126d47a784dabb10064019c14cc63</td>\n",
       "      <td>2018-06-28 20:41:23.282481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFO</td>\n",
       "      <td>acebdf79-34a8-4866-9399-a9bad449b725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W/\"datetime'2018-06-28T20%3A31%3A26.0426356Z'\"</td>\n",
       "      <td>Simulation started.</td>\n",
       "      <td>Machine-001</td>\n",
       "      <td>2446526a166143f89803ff1e7fe3ec8a</td>\n",
       "      <td>2018-06-28 20:31:26.042635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFO</td>\n",
       "      <td>7ae0038c-3c64-47bb-8dba-6789980ef560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W/\"datetime'2018-06-28T18%3A09%3A07.0027293Z'\"</td>\n",
       "      <td>Simulation started.</td>\n",
       "      <td>Machine-001</td>\n",
       "      <td>6270bed934a244bab651ca74279271c7</td>\n",
       "      <td>2018-06-28 18:09:07.002729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFO</td>\n",
       "      <td>9c8b76a2-d097-4301-ab3a-3f716bf9f1ac</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W/\"datetime'2018-06-28T20%3A04%3A24.180131Z'\"</td>\n",
       "      <td>Simulation started.</td>\n",
       "      <td>Machine-001</td>\n",
       "      <td>73709a650c044fb998dc1b36a5295de3</td>\n",
       "      <td>2018-06-28 20:04:24.180131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Level                               _Driver Code  \\\n",
       "0  INFO  8621bba7-55e2-485f-b191-c7365d9f5847  NaN   \n",
       "1  INFO  c465fc84-4710-4ac3-ae77-5648037b3cfe  NaN   \n",
       "2  INFO  acebdf79-34a8-4866-9399-a9bad449b725  NaN   \n",
       "3  INFO  7ae0038c-3c64-47bb-8dba-6789980ef560  NaN   \n",
       "4  INFO  9c8b76a2-d097-4301-ab3a-3f716bf9f1ac  NaN   \n",
       "\n",
       "                                             etag              Message  \\\n",
       "0  W/\"datetime'2018-06-28T18%3A09%3A07.0277527Z'\"  Simulation started.   \n",
       "1  W/\"datetime'2018-06-28T20%3A41%3A23.2824814Z'\"  Simulation started.   \n",
       "2  W/\"datetime'2018-06-28T20%3A31%3A26.0426356Z'\"  Simulation started.   \n",
       "3  W/\"datetime'2018-06-28T18%3A09%3A07.0027293Z'\"  Simulation started.   \n",
       "4   W/\"datetime'2018-06-28T20%3A04%3A24.180131Z'\"  Simulation started.   \n",
       "\n",
       "  PartitionKey                            RowKey                  Timestamp  \n",
       "0  Machine-000  43a1504005d2467694a8f7b8b73888fb 2018-06-28 18:09:07.027752  \n",
       "1  Machine-001  105126d47a784dabb10064019c14cc63 2018-06-28 20:41:23.282481  \n",
       "2  Machine-001  2446526a166143f89803ff1e7fe3ec8a 2018-06-28 20:31:26.042635  \n",
       "3  Machine-001  6270bed934a244bab651ca74279271c7 2018-06-28 18:09:07.002729  \n",
       "4  Machine-001  73709a650c044fb998dc1b36a5295de3 2018-06-28 20:04:24.180131  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = list()\n",
    "for row in tblob:\n",
    "    if (len(attributes) == 0):\n",
    "        for attribute in row:\n",
    "            attributes.append(attribute)\n",
    "    break\n",
    "log_df = pd.DataFrame(columns=attributes)\n",
    "for row in tblob:\n",
    "    if (row[\"Level\"] != \"DEBUG\"):\n",
    "        row_dict = {}    \n",
    "        for attribute in row:\n",
    "            if (attribute != \"Timestamp\"):\n",
    "                row_dict[attribute] = row[attribute]\n",
    "            else:\n",
    "                newtime = row[attribute].replace(tzinfo=None)\n",
    "                timeitem = pd.Timestamp(newtime, tz=None)\n",
    "                row_dict[attribute] = timeitem\n",
    "        log_df = log_df.append(row_dict, ignore_index=True)\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Run-To-Failure Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Run-to-Failures: 2\n"
     ]
    }
   ],
   "source": [
    "message_counts = log_df['Message'].value_counts()\n",
    "if ('failure' in message_counts):\n",
    "    print(\"Number of Run-to-Failures:\", message_counts['failure'])\n",
    "else:\n",
    "    raise ValueError('Run to failure count is 0. Do not proceed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select necessary attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>code</th>\n",
       "      <th>level</th>\n",
       "      <th>machineID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-06-28 18:09:07.027752</th>\n",
       "      <td>2018-06-28 18:09:07.027752</td>\n",
       "      <td>nan</td>\n",
       "      <td>INFO</td>\n",
       "      <td>Machine-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-28 20:41:23.282481</th>\n",
       "      <td>2018-06-28 20:41:23.282481</td>\n",
       "      <td>nan</td>\n",
       "      <td>INFO</td>\n",
       "      <td>Machine-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-28 20:31:26.042635</th>\n",
       "      <td>2018-06-28 20:31:26.042635</td>\n",
       "      <td>nan</td>\n",
       "      <td>INFO</td>\n",
       "      <td>Machine-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-28 18:09:07.002729</th>\n",
       "      <td>2018-06-28 18:09:07.002729</td>\n",
       "      <td>nan</td>\n",
       "      <td>INFO</td>\n",
       "      <td>Machine-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-28 20:04:24.180131</th>\n",
       "      <td>2018-06-28 20:04:24.180131</td>\n",
       "      <td>nan</td>\n",
       "      <td>INFO</td>\n",
       "      <td>Machine-001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             timestamp code level    machineID\n",
       "timestamp                                                                     \n",
       "2018-06-28 18:09:07.027752  2018-06-28 18:09:07.027752  nan  INFO  Machine-000\n",
       "2018-06-28 20:41:23.282481  2018-06-28 20:41:23.282481  nan  INFO  Machine-001\n",
       "2018-06-28 20:31:26.042635  2018-06-28 20:31:26.042635  nan  INFO  Machine-001\n",
       "2018-06-28 18:09:07.002729  2018-06-28 18:09:07.002729  nan  INFO  Machine-001\n",
       "2018-06-28 20:04:24.180131  2018-06-28 20:04:24.180131  nan  INFO  Machine-001"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df = log_df[[\"Timestamp\", \"Code\", \"Level\", \"PartitionKey\"]].astype(str)\n",
    "log_df.columns = [\"timestamp\", \"code\",\"level\",\"machineID\"]\n",
    "log_df.index = log_df['timestamp']\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write logs to system storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = sqlContext.createDataFrame(log_df)\n",
    "log_df.write.parquet(data_dir+\"/logs\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
