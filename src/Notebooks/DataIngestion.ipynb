{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industrial Machinery Data Ingestor for IoT Data\n",
    "\n",
    "## Introduction\n",
    "Implementing a production IoT Predictive Maintenance solution requires having mature remote condition monitoring infrastructure in place. More specifically, data-driven prognostics hinge on the availability of statistically significant amounts of run-to-failure telemetry and maintenance records, from which equipment degradation patterns can be learned to enable failure predications based on both historical and newly collected data.\n",
    "Real-world run-to-failure data sets are virtually impossible to come across due to their commercially sensitive nature. Of the several publicly available synthetic data sets[1][2], none ideally fits the canonical IoT scenario in which highly irregular real-time data streams from sensors are captured and used for condition monitoring or anomaly detection[3]. For instance, the tiny Turbofan Engine Degradation Simulation Data Set [1] created with the commercial version of the Modular Aero-Propulsion System Simulation environment (C-MAPSS) [4] contains a single telemetry snapshot per operational cycle of unspecified length, whereas the data set which accompanies the Predictive Maintenance Modelling Guide [2] features hourly, presumably aggregated, sensor readings produced by making statistical alterations to uncorrelated series of random numbers. Unfortunately, the source code used for generating these data sets is not publicly available (in fact, obtaining C-MAPSS requires a grant from the U.S. Government). This makes the tasks of converting these data sets to a format which bears more resemblance to that of an IoT-centered scenario and generating new customized data for real-time simulation rather difficult.\n",
    "\n",
    "## Notebook Purpose\n",
    "This notebook ingests and prepares the IoT data being collected and stored in the PdM solution deployment for further use in FeatureEngineering and Model Training. It ingests both sensor telemetry data as well as logging output from IoT devices.\n",
    "\n",
    "## Data inventory\n",
    "The ingested data set will be a combination of maintenance and telemetry records. It is assumed that sensor data, along with the values describing a machine's current operational settings (in this case, rotational speed), is periodically transmitted from an IoT Edge device, with or without preprocessing, to the cloud where it is used for predictive analysis.\n",
    "### Maintenance and failure records\n",
    "In the present implementation, the maintenance data set will contain primarily failure events indicating exact points in time when a machine had a critical failure of a particular type. The intent of Predictive Maintenance is preventing these events by raising alarms in advance, so that appropriate preventive activities can be carried out.\n",
    "#### Format\n",
    "* timestamp\n",
    "* level (INFO, WARNING, ERROR, CRITICAL)\n",
    "* machine ID\n",
    "* code (identifies event/failure type)\n",
    "* message (contains additional details)\n",
    "### Telemetry\n",
    "This data set contains an IoT telemetry stream.\n",
    "#### Format\n",
    "* timestamp\n",
    "* machine ID\n",
    "* ambient temperature (°C)\n",
    "* ambient pressure (kPa)\n",
    "* rotational speed (RPM)\n",
    "* temperature (°C)\n",
    "* pressure (kPa)\n",
    "* vibration signal\n",
    "\n",
    "### Operational settings, operational conditions and performance\n",
    "Operational settings determine the mode of operation and have a substantial effect on the observed performance and other monitored phenomena. For rotational equipment, one of the possible operational settings is the \"desired\" speed expressed in rotations per minute (RPM). Other operational settings may come into play in real-world scenarios. (For instance, the Turbofan Engine Degradation Simulation Data Set [1] contains three operational settings.)\n",
    "Operational conditions define the environment in which equipment is being operated. Weather conditions, location, characteristics of the operator are some of the examples. Operational conditions may impact the performance of the equipment and therefore should be taken into account in predictive modeling.\n",
    "Performance is determined by current operational settings, operational conditions and physical state of the equipment (e.g., wear). In this example, performance is expressed as a set of the following sensor measurements:\n",
    "* speed (actual)\n",
    "* temperature\n",
    "* pressure\n",
    "* vibration\n",
    "\n",
    "Depending on the type of the equipment, some sensors will measure useful output, and some the side effects of mechanical or other processes (or, in energy terms, the loss). Most of the time, upcoming failures manifest themselves in a gradually diminishing useful output and increased loss under some or all operational settings; for example, assuming that pressure is considered \"useful output,\" a machine operating at the same speed would generate increasingly less pressure while, possibly, also producing more heat or vibration. Performance measurements often exhibit some complex nonlinear behavior with respect to operational settings, operational conditions and equipment health.\n",
    "The general idea behind Predictive Maintenance is that different types of impending failures manifest themselves in different ways over time, and that such patterns can be learned given sufficient amount of collected data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Importing and Environment Variable Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType, StringType\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from azure.storage.table import TableService"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For development purposes only until ENV Variables get set\n",
    "from pathlib import Path\n",
    "env_config_file_location = (str(Path.home())+\"/NotebookEnvironmentVariablesConfig.json\")\n",
    "config_file = Path(env_config_file_location)\n",
    "if not config_file.is_file():\n",
    "  env_config_file_location = (\"/dbfs\"+str(Path.home())+\"/NotebookEnvironmentVariablesConfig.json\")\n",
    "f = open(env_config_file_location)\n",
    "env_variables = json.load(f)[\"DataIngestion\"]\n",
    "\n",
    "STORAGE_ACCOUNT_SUFFIX = 'core.windows.net'\n",
    "STORAGE_ACCOUNT_NAME = env_variables[\"STORAGE_ACCOUNT_NAME\"]\n",
    "STORAGE_ACCOUNT_KEY = env_variables[\"STORAGE_ACCOUNT_KEY\"]\n",
    "TELEMETRY_CONTAINER_NAME = env_variables[\"TELEMETRY_CONTAINER_NAME\"]\n",
    "LOG_TABLE_NAME = env_variables[\"LOG_TABLE_NAME\"]\n",
    "DATA_ROOT = env_variables[\"DATA_ROOT_FOLDER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Ingested Data Drop Folder\n",
    "This location is where the prepared ingested IoT data is stored for further use in the notebooks to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = DATA_ROOT + '/data'\n",
    "\n",
    "#TODO: Convert data_dir into env variable\n",
    "% rm -rf $data_dir\n",
    "% mkdir $data_dir $data_dir/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving telemetry data\n",
    "The raw data retrieved from the PdM solution storage contains all the IoT telemetry data in the \"Body\" column of the dataframe in a byte array. It needs to be deserialized into a string representing JSON, then expanded into a separate dataframe to be used by FeatureEngineering and ModelTraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wasbTelemetryUrl = \"wasb://{0}@{1}.blob.{2}/*/*/*/*/*/*/*\".format(TELEMETRY_CONTAINER_NAME, \n",
    "                                                                  STORAGE_ACCOUNT_NAME, \n",
    "                                                                  STORAGE_ACCOUNT_SUFFIX)\n",
    "\n",
    "sc = SparkSession.builder.getOrCreate()\n",
    "hc = sc._jsc.hadoopConfiguration()\n",
    "hc.set(\"avro.mapred.ignore.inputs.without.extension\", \"false\")\n",
    "if STORAGE_ACCOUNT_KEY:\n",
    "     hc.set(\"fs.azure.account.key.{}.blob.core.windows.net\".format(STORAGE_ACCOUNT_NAME), STORAGE_ACCOUNT_KEY)\n",
    "hc.set(\"fs.azure.account.key.{}.blob.core.windows.net\"\n",
    "    .format(STORAGE_ACCOUNT_NAME), STORAGE_ACCOUNT_KEY)\n",
    "sql = SQLContext.getOrCreate(sc)\n",
    "avroblob = sql.read.format(\"com.databricks.spark.avro\").load(wasbTelemetryUrl)\n",
    "avroblob.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert byteformatted \"body\" of raw blob data into JSON, explode result into new Pyspark DataFrame\n",
    "The output here shows the schema of the telemetry data as well as a preview of the telemetry data with the specific columns necessary for FeatureEngineering and ModelTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert byteformat to string format in pyspark dataframe\n",
    "from json import loads as Loads\n",
    "column = avroblob['Body']\n",
    "string_udf = udf(lambda x: x.decode(\"utf-8\"))\n",
    "avroblob=avroblob.withColumn(\"BodyString\", string_udf(column))\n",
    "avroblob.printSchema()\n",
    "\n",
    "#Convert \"body\" into new DataFrame\n",
    "telemetry_df = sql.read.json(avroblob.select(\"BodyString\").rdd.map(lambda r: r.BodyString))\n",
    "subsetted_df = telemetry_df.select([\"timestamp\", \"ambient_pressure\",\"ambient_temperature\",\"machineID\",\"pressure\",\"speed\",\"speed_desired\",\"temperature\"])\n",
    "subsetted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "e = '%Y-%m-%dT%H:%M:%S.%f'\n",
    "reformatted_time_df = subsetted_df.withColumn(\"timestamp\", F.col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "reformatted_time_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataframe to Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_time_df.write.parquet(data_dir+\"/telemetry\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table retrieval\n",
    "table_service = TableService(account_name=STORAGE_ACCOUNT_NAME, account_key=STORAGE_ACCOUNT_KEY)\n",
    "tblob = table_service.query_entities(LOG_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process log table data into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = list()\n",
    "for row in tblob:\n",
    "    if (len(attributes) == 0):\n",
    "        for attribute in row:\n",
    "            attributes.append(attribute)\n",
    "    break\n",
    "log_df = pd.DataFrame(columns=attributes)\n",
    "for row in tblob:\n",
    "    if (row[\"Level\"] != \"DEBUG\"):\n",
    "        row_dict = {}    \n",
    "        for attribute in row:\n",
    "            if (attribute != \"Timestamp\"):\n",
    "                row_dict[attribute] = row[attribute]\n",
    "            else:\n",
    "                newtime = row[attribute].replace(tzinfo=None)\n",
    "                timeitem = pd.Timestamp(newtime, tz=None)\n",
    "                row_dict[attribute] = timeitem\n",
    "        log_df = log_df.append(row_dict, ignore_index=True)\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Run-To-Failure Sequences\n",
    "The number of Run-To-Failure sequences is especially important for FeatureEngineering and ModelTraining as these log instances are used to train the predictive model. If there are no failure sequences logged, then training a predictive model is useless as the model has no reference for what a situation for failure may look like. Do not proceed with the notebooks if there are no Run-To-Failure sequences logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_counts = log_df['Message'].value_counts()\n",
    "if ('failure' in message_counts):\n",
    "    print(\"Number of Run-to-Failures:\", message_counts['failure'])\n",
    "else:\n",
    "    raise ValueError('Run to failure count is 0. Do not proceed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select necessary attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = log_df[[\"Timestamp\", \"Code\", \"Level\", \"PartitionKey\"]].astype(str)\n",
    "log_df.columns = [\"timestamp\", \"code\",\"level\",\"machineID\"]\n",
    "log_df.index = log_df['timestamp']\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write logs to system storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = sqlContext.createDataFrame(log_df)\n",
    "log_df.write.parquet(data_dir+\"/logs\", mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
