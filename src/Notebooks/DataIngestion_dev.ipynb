{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Avro Blobs Into Parquet Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Importing and Environment Variable Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType, StringType\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from azure.storage.table import TableService\n",
    "\n",
    "STORAGE_ACCOUNT_SUFFIX = 'core.windows.net'\n",
    "STORAGE_ACCOUNT_NAME = os.getenv('TELEMETRY_STORAGE_ACCOUNT_NAME')\n",
    "STORAGE_ACCOUNT_KEY = os.getenv('TELEMETRY_STORAGE_ACCOUNT_KEY')\n",
    "TELEMETRY_CONTAINER_NAME = os.getenv('TELEMETRY_CONTAINER_NAME')\n",
    "LOG_TABLE_NAME = 'logs'\n",
    "LOG_TABLE_NAME = os.getenv('LOG_TABLE_NAME') #TODO need to add this to ENV Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Development Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For development purposes only until ENV Variables get set\n",
    "STORAGE_ACCOUNT_SUFFIX = 'core.windows.net'\n",
    "STORAGE_ACCOUNT_NAME = \"stgmosxhnn5t3nyc\"\n",
    "STORAGE_ACCOUNT_KEY = \"fa0TlK+KYvXfLXGqfqVATbNF4xB5o64O4i7PjmlGMSTXCLMIOh/9Sc4ScpJ2V0vUzQ2TlK4wVu0BbA9i3HZGaw==\"\n",
    "TELEMETRY_CONTAINER_NAME = \"telemetry\"\n",
    "LOG_TABLE_NAME = 'logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Drop Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_dir = str(Path.home()) + '/data'\n",
    "\n",
    "#TODO: Convert data_dir into env variable\n",
    "% rm -rf $data_dir\n",
    "% mkdir $data_dir $data_dir/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving telemetry data (as spark dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+\n",
      "|     EnqueuedTimeUtc|Properties|    SystemProperties|                Body|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "|2018-06-26T00:45:...|     Map()|Map(connectionAut...|[7B 22 6D 61 63 6...|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wasbTelemetryUrl = \"wasb://{0}@{1}.blob.{2}/*/*/*/*/*/*/*\".format(TELEMETRY_CONTAINER_NAME, \n",
    "                                                                  STORAGE_ACCOUNT_NAME, \n",
    "                                                                  STORAGE_ACCOUNT_SUFFIX)\n",
    "\n",
    "sc = SparkSession.builder.getOrCreate()\n",
    "hc = sc._jsc.hadoopConfiguration()\n",
    "hc.set(\"avro.mapred.ignore.inputs.without.extension\", \"false\")\n",
    "if STORAGE_ACCOUNT_KEY:\n",
    "     hc.set(\"fs.azure.account.key.{}.blob.core.windows.net\".format(STORAGE_ACCOUNT_NAME), STORAGE_ACCOUNT_KEY)\n",
    "hc.set(\"fs.azure.account.key.{}.blob.core.windows.net\"\n",
    "    .format(STORAGE_ACCOUNT_NAME), STORAGE_ACCOUNT_KEY)\n",
    "sql = SQLContext.getOrCreate(sc)\n",
    "avroblob = sql.read.format(\"com.databricks.spark.avro\").load(wasbTelemetryUrl)\n",
    "avroblob.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert byteformatted \"body\" of raw blob data into JSON, then explode result into new Pyspark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EnqueuedTimeUtc: string (nullable = true)\n",
      " |-- Properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- SystemProperties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- Body: binary (nullable = true)\n",
      " |-- BodyString: string (nullable = true)\n",
      "\n",
      "+----------------+-------------------+-----------+--------+-------+-------------+-----------+--------------------+---------+\n",
      "|ambient_pressure|ambient_temperature|  machineID|pressure|  speed|speed_desired|temperature|           timestamp|vibration|\n",
      "+----------------+-------------------+-----------+--------+-------+-------------+-----------+--------------------+---------+\n",
      "|          100.96|              19.95|Machine-003| 1448.84| 1078.2|         1000|     135.51|2018-06-26T00:45:...|     null|\n",
      "|           101.0|              20.06|Machine-004|  868.15|1030.75|         1000|     151.18|2018-06-26T00:45:...|     null|\n",
      "|          100.91|              20.05|Machine-003| 1483.13| 1077.5|         1000|     135.62|2018-06-26T00:45:...|     null|\n",
      "|          101.06|              20.02|Machine-003| 1450.96|1069.84|         1000|     135.47|2018-06-26T00:45:...|     null|\n",
      "|          100.96|              19.92|Machine-003| 1480.05| 1070.1|         1000|     135.54|2018-06-26T00:45:...|     null|\n",
      "|          100.92|              20.02|Machine-003| 1473.09|1071.97|         1000|     135.54|2018-06-26T00:45:...|     null|\n",
      "|          101.08|              20.09|Machine-003| 1425.59|1068.51|         1000|     135.59|2018-06-26T00:45:...|     null|\n",
      "|          101.07|              20.04|Machine-003| 1485.35|1069.83|         1000|     135.53|2018-06-26T00:45:...|     null|\n",
      "|          101.02|               20.1|Machine-003| 1450.46|1076.48|         1000|     135.58|2018-06-26T00:45:...|     null|\n",
      "|          101.09|              19.95|Machine-003| 1448.98|1076.46|         1000|     135.49|2018-06-26T00:45:...|     null|\n",
      "|          100.97|               20.1|Machine-004|  918.73|1044.85|         1000|     151.27|2018-06-26T00:45:...|     null|\n",
      "|          101.04|               20.0|Machine-003| 1474.24|1070.19|         1000|     135.59|2018-06-26T00:45:...|     null|\n",
      "|          100.98|              20.02|Machine-003|  1464.3|1076.91|         1000|      135.6|2018-06-26T00:45:...|     null|\n",
      "|           100.9|              20.07|Machine-003| 1474.43|1070.61|         1000|     135.44|2018-06-26T00:45:...|     null|\n",
      "|          101.09|              20.05|Machine-003| 1450.21|1069.69|         1000|     135.65|2018-06-26T00:45:...|     null|\n",
      "|          100.94|              19.94|Machine-003| 1447.88|1068.97|         1000|     135.65|2018-06-26T00:45:...|     null|\n",
      "|          100.98|              20.06|Machine-003| 1462.75|1076.68|         1000|     135.54|2018-06-26T00:45:...|     null|\n",
      "|          101.05|              19.97|Machine-003| 1462.76|1072.18|         1000|     135.62|2018-06-26T00:45:...|     null|\n",
      "|          100.98|              20.06|Machine-004|  952.45| 1053.9|         1000|     151.31|2018-06-26T00:45:...|     null|\n",
      "|          100.97|              20.09|Machine-003| 1476.37| 1075.0|         1000|     135.52|2018-06-26T00:45:...|     null|\n",
      "+----------------+-------------------+-----------+--------+-------+-------------+-----------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert byteformat to string format in pyspark dataframe\n",
    "from json import loads as Loads\n",
    "column = avroblob['Body']\n",
    "string_udf = udf(lambda x: x.decode(\"utf-8\"))\n",
    "avroblob=avroblob.withColumn(\"BodyString\", string_udf(column))\n",
    "avroblob.printSchema()\n",
    "\n",
    "#Convert \"body\" into new DataFrame\n",
    "telemetry_df = sql.read.json(avroblob.select(\"BodyString\").rdd.map(lambda r: r.BodyString))\n",
    "telemetry_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns to retain: timestamp, ambient_pressure, ambient_temperature machineID, pressure, speed, \n",
    "#                   speed_desired, temperature\n",
    "subsetted_df = telemetry_df.select([\"timestamp\", \"ambient_pressure\",\"ambient_temperature\",\"machineID\",\"pressure\",\"speed\",\"speed_desired\",\"temperature\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- ambient_pressure: double (nullable = true)\n",
      " |-- ambient_temperature: double (nullable = true)\n",
      " |-- machineID: string (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- speed_desired: long (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#modify timestamp format\n",
    "import datetime\n",
    "e = '%Y-%m-%dT%H:%M:%S.%f'\n",
    "timestamp_udf = udf(lambda date: datetime.datetime.strptime(date, e), TimestampType())\n",
    "reformatted_time_df = subsetted_df.withColumn(\"timestamp\", timestamp_udf(subsetted_df[\"timestamp\"]))\n",
    "\n",
    "reformatted_time_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataframe to Parquet in system storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_time_df.write.parquet(data_dir+\"/telemetry\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table retrieval\n",
    "table_service = TableService(account_name=STORAGE_ACCOUNT_NAME, account_key=STORAGE_ACCOUNT_KEY)\n",
    "tblob = table_service.query_entities(LOG_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process log table data into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>PartitionKey</th>\n",
       "      <th>Level</th>\n",
       "      <th>etag</th>\n",
       "      <th>Message</th>\n",
       "      <th>RowKey</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>_Driver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Machine-000</td>\n",
       "      <td>INFO</td>\n",
       "      <td>W/\"datetime'2018-06-27T01%3A48%3A27.276968Z'\"</td>\n",
       "      <td>Simulation started.</td>\n",
       "      <td>2ee434dc37e846919721511323c93153</td>\n",
       "      <td>2018-06-27 01:48:27.276968</td>\n",
       "      <td>2155f98b-1e3b-495a-b5b9-451cb87c0f1d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Machine-000</td>\n",
       "      <td>INFO</td>\n",
       "      <td>W/\"datetime'2018-06-27T01%3A58%3A39.0637119Z'\"</td>\n",
       "      <td>Simulation started.</td>\n",
       "      <td>4ca50e21e49a4fc7883bfbb557482e98</td>\n",
       "      <td>2018-06-27 01:58:39.063711</td>\n",
       "      <td>1ab7eae1-fb35-419c-9485-b644280a2a23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Machine-000</td>\n",
       "      <td>INFO</td>\n",
       "      <td>W/\"datetime'2018-06-27T03%3A28%3A41.3005356Z'\"</td>\n",
       "      <td>Simulation started.</td>\n",
       "      <td>5768022591c44433893532a1fc0ea075</td>\n",
       "      <td>2018-06-27 03:28:41.300535</td>\n",
       "      <td>3fe41f9e-9ba0-4b9d-b072-fc4364f256f3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Machine-000</td>\n",
       "      <td>INFO</td>\n",
       "      <td>W/\"datetime'2018-06-27T04%3A02%3A15.5696863Z'\"</td>\n",
       "      <td>Simulation started.</td>\n",
       "      <td>585d6262305240b5987eb235845e9dad</td>\n",
       "      <td>2018-06-27 04:02:15.569686</td>\n",
       "      <td>7d5806ca-5e02-498a-a599-fd296b70bac6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Machine-000</td>\n",
       "      <td>INFO</td>\n",
       "      <td>W/\"datetime'2018-06-27T03%3A50%3A16.5535432Z'\"</td>\n",
       "      <td>Simulation started.</td>\n",
       "      <td>5b692f03bc214d128905564f1fd9f2a2</td>\n",
       "      <td>2018-06-27 03:50:16.553543</td>\n",
       "      <td>dbaa3b25-8cc1-4c90-b509-f751277a26d1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Code PartitionKey Level                                            etag  \\\n",
       "0  NaN  Machine-000  INFO   W/\"datetime'2018-06-27T01%3A48%3A27.276968Z'\"   \n",
       "1  NaN  Machine-000  INFO  W/\"datetime'2018-06-27T01%3A58%3A39.0637119Z'\"   \n",
       "2  NaN  Machine-000  INFO  W/\"datetime'2018-06-27T03%3A28%3A41.3005356Z'\"   \n",
       "3  NaN  Machine-000  INFO  W/\"datetime'2018-06-27T04%3A02%3A15.5696863Z'\"   \n",
       "4  NaN  Machine-000  INFO  W/\"datetime'2018-06-27T03%3A50%3A16.5535432Z'\"   \n",
       "\n",
       "               Message                            RowKey  \\\n",
       "0  Simulation started.  2ee434dc37e846919721511323c93153   \n",
       "1  Simulation started.  4ca50e21e49a4fc7883bfbb557482e98   \n",
       "2  Simulation started.  5768022591c44433893532a1fc0ea075   \n",
       "3  Simulation started.  585d6262305240b5987eb235845e9dad   \n",
       "4  Simulation started.  5b692f03bc214d128905564f1fd9f2a2   \n",
       "\n",
       "                   Timestamp                               _Driver  \n",
       "0 2018-06-27 01:48:27.276968  2155f98b-1e3b-495a-b5b9-451cb87c0f1d  \n",
       "1 2018-06-27 01:58:39.063711  1ab7eae1-fb35-419c-9485-b644280a2a23  \n",
       "2 2018-06-27 03:28:41.300535  3fe41f9e-9ba0-4b9d-b072-fc4364f256f3  \n",
       "3 2018-06-27 04:02:15.569686  7d5806ca-5e02-498a-a599-fd296b70bac6  \n",
       "4 2018-06-27 03:50:16.553543  dbaa3b25-8cc1-4c90-b509-f751277a26d1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = list()\n",
    "for row in tblob:\n",
    "    if (len(attributes) == 0):\n",
    "        for attribute in row:\n",
    "            attributes.append(attribute)\n",
    "    break\n",
    "log_df = pd.DataFrame(columns=attributes)\n",
    "for row in tblob:\n",
    "    if (row[\"Level\"] != \"DEBUG\"):\n",
    "        row_dict = {}    \n",
    "        for attribute in row:\n",
    "            if (attribute != \"Timestamp\"):\n",
    "                row_dict[attribute] = row[attribute]\n",
    "            else:\n",
    "                newtime = row[attribute].replace(tzinfo=None)\n",
    "                timeitem = pd.Timestamp(newtime, tz=None)\n",
    "                row_dict[attribute] = timeitem\n",
    "        log_df = log_df.append(row_dict, ignore_index=True)\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Run-To-Failure Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = log_df[log_df.Message != 'failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Run to failure count is 0\n"
     ]
    }
   ],
   "source": [
    "message_counts = log_df['Message'].value_counts()\n",
    "if ('failure' in message_counts):\n",
    "    print(\"Number of Run-to-Failures:\", message_counts['failure'])\n",
    "else:\n",
    "    print(\"WARNING: Run to failure count is 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select necessary attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>level</th>\n",
       "      <th>machineID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-06-27 01:48:27.276968</th>\n",
       "      <td>NaN</td>\n",
       "      <td>INFO</td>\n",
       "      <td>Machine-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-27 01:58:39.063711</th>\n",
       "      <td>NaN</td>\n",
       "      <td>INFO</td>\n",
       "      <td>Machine-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-27 03:28:41.300535</th>\n",
       "      <td>NaN</td>\n",
       "      <td>INFO</td>\n",
       "      <td>Machine-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-27 04:02:15.569686</th>\n",
       "      <td>NaN</td>\n",
       "      <td>INFO</td>\n",
       "      <td>Machine-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-27 03:50:16.553543</th>\n",
       "      <td>NaN</td>\n",
       "      <td>INFO</td>\n",
       "      <td>Machine-000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           code level    machineID\n",
       "timestamp                                         \n",
       "2018-06-27 01:48:27.276968  NaN  INFO  Machine-000\n",
       "2018-06-27 01:58:39.063711  NaN  INFO  Machine-000\n",
       "2018-06-27 03:28:41.300535  NaN  INFO  Machine-000\n",
       "2018-06-27 04:02:15.569686  NaN  INFO  Machine-000\n",
       "2018-06-27 03:50:16.553543  NaN  INFO  Machine-000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df = log_df[[\"Timestamp\", \"Code\", \"Level\", \"PartitionKey\"]]\n",
    "log_df.columns = [\"timestamp\", \"code\",\"level\",\"machineID\"]\n",
    "log_df.index = log_df['timestamp']\n",
    "del log_df['timestamp']\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write logs to system storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.to_parquet(data_dir+\"/logs/logs.parquet\", engine='fastparquet', times='int96')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
