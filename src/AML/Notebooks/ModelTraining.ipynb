{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In the previous notebook, we performed featurization on the raw telemetry dataset and, using failure records from the event logs, assigned a failure class to each of its entries (with 'None' signifying a non-failure). In what follows, we will be training the Random Forest classifier<sup>[[1]](#ref_1)</sup> from Spark MLlib. (The use of Random Forest and its tuning parameters were adopted from [this sample](https://github.com/Azure/MachineLearningSamples-PredictiveMaintenance/blob/master/Code/3_model_building.ipynb).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf, mean, lit, stddev, col, expr, when\n",
    "from pyspark.sql.types import DoubleType, ArrayType, ShortType, LongType, IntegerType\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer, IndexToString\n",
    "\n",
    "STORAGE_ACCOUNT_SUFFIX = 'core.windows.net'\n",
    "STAGING_STORAGE_ACCOUNT_NAME = os.getenv('STAGING_STORAGE_ACCOUNT_NAME')\n",
    "STAGING_STORAGE_ACCOUNT_KEY = os.getenv('STAGING_STORAGE_ACCOUNT_KEY')\n",
    "AZUREML_NATIVE_SHARE_DIRECTORY = os.getenv('AZUREML_NATIVE_SHARE_DIRECTORY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the feature data with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- machineID: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- ambient_temperature: double (nullable = true)\n",
      " |-- ambient_pressure: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- f0: double (nullable = true)\n",
      " |-- f1: double (nullable = true)\n",
      " |-- f2: double (nullable = true)\n",
      " |-- a0: double (nullable = true)\n",
      " |-- a1: double (nullable = true)\n",
      " |-- a2: double (nullable = true)\n",
      " |-- temperature_n: double (nullable = true)\n",
      " |-- pressure_n: double (nullable = true)\n",
      " |-- f0_n: double (nullable = true)\n",
      " |-- f1_n: double (nullable = true)\n",
      " |-- f2_n: double (nullable = true)\n",
      " |-- a0_n: double (nullable = true)\n",
      " |-- a1_n: double (nullable = true)\n",
      " |-- a2_n: double (nullable = true)\n",
      " |-- failure: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = SparkSession.builder.getOrCreate()\n",
    "hc = sc._jsc.hadoopConfiguration()\n",
    "hc.set(\"avro.mapred.ignore.inputs.without.extension\", \"false\")\n",
    "\n",
    "hc.set(\"fs.azure.account.key.{}.blob.core.windows.net\".format(STAGING_STORAGE_ACCOUNT_NAME), STAGING_STORAGE_ACCOUNT_KEY)\n",
    "\n",
    "sql = SQLContext.getOrCreate(sc)\n",
    "\n",
    "wasbUrlOutput = \"wasb://{0}@{1}.blob.{2}/features.parquet\".format(\n",
    "            'intermediate',\n",
    "            STAGING_STORAGE_ACCOUNT_NAME,\n",
    "            STORAGE_ACCOUNT_SUFFIX)\n",
    "\n",
    "labeled_features_df = sql.read.parquet(wasbUrlOutput)\n",
    "labeled_features_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming individual features into a vector column\n",
    "\n",
    "We are using *VectorAssembler* here to combine all the features into a single feature vector. Vectors is what ML models like logistic regression and decision trees expect as their input. Note that we also alphabetically sort the names of the feature columns when passing them to *VectorAssembler* so that it's easier to identify individual elements in the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sorted = sorted([c for c in labeled_features_df.columns if c not in ['machineID', 'timestamp', 'failure']])\n",
    "va = VectorAssembler(inputCols=features_sorted, outputCol='features')\n",
    "vectorized_features_df = va.transform(labeled_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureIndexer = VectorIndexer(inputCol=\"features\", \n",
    "                               outputCol=\"indexedFeatures\", \n",
    "                               maxCategories=10).fit(vectorized_features_df)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"failure\", outputCol=\"indexedLabel\").fit(vectorized_features_df)\n",
    "\n",
    "deIndexer = IndexToString(inputCol = \"prediction\", outputCol = \"predictedFailure\", labels = labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80/20 Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 161866 records\n",
      "Test dataset: 40609 records\n"
     ]
    }
   ],
   "source": [
    "training, test = vectorized_features_df.randomSplit([0.8, 0.2], seed=12345)\n",
    "print('Training dataset: {0} records'.format(training.count()))\n",
    "print('Test dataset: {0} records'.format(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(labelCol=\"indexedLabel\",\n",
    "                                    featuresCol=\"indexedFeatures\",\n",
    "                                    maxDepth=15,\n",
    "                                    maxBins=32,\n",
    "                                    minInstancesPerNode=1,\n",
    "                                    minInfoGain=0.0,\n",
    "                                    impurity=\"gini\",\n",
    "                                    numTrees=50,\n",
    "                                    featureSubsetStrategy=\"sqrt\",\n",
    "                                    subsamplingRate = 0.632)\n",
    " \n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, classifier, deIndexer])\n",
    "\n",
    "fitted_pipeline = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing (confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>failure_predictedFailure</th>\n",
       "      <th>F01</th>\n",
       "      <th>F02</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F01</td>\n",
       "      <td>4099</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F02</td>\n",
       "      <td>0</td>\n",
       "      <td>3958</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>32376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  failure_predictedFailure   F01   F02   None\n",
       "0                      F01  4099     0      0\n",
       "2                      F02     0  3958    123\n",
       "1                     None     0    53  32376"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = fitted_pipeline.transform(test)\n",
    "conf_table = predictions.stat.crosstab('failure', 'predictedFailure')\n",
    "confuse = conf_table.toPandas().sort_values(by=['failure_predictedFailure'])\n",
    "confuse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(AZUREML_NATIVE_SHARE_DIRECTORY, 'model')\n",
    "model_archive_path = os.path.join(AZUREML_NATIVE_SHARE_DIRECTORY, 'model.tar.gz')\n",
    "\n",
    "fitted_pipeline.write().overwrite().save(model_path)\n",
    "\n",
    "import tarfile\n",
    "\n",
    "tar = tarfile.open(model_archive_path, \"w:gz\")\n",
    "tar.add(model_path, arcname=\"model\")\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a name=\"ref_1\"></a>1.  [Random Forests](https://spark.apache.org/docs/latest/mllib-ensembles.html#random-forests). \n",
    "Machine Learning Library (MLlib) Guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
