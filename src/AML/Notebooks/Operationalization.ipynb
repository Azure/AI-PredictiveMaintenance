{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ambient_pressure: double (nullable = true)\n",
      " |-- ambient_temperature: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- vibration: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-current/python/pyspark/sql/session.py:331: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "with open('../service_schema.json') as f:\n",
    "    schema = json.loads(f.read())\n",
    "\n",
    "input = schema['input']['input_df']['swagger']['example']\n",
    "\n",
    "input_df = sqlContext.createDataFrame(input)\n",
    "\n",
    "input_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../score.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql.types import DoubleType, ArrayType, ShortType, LongType, IntegerType\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "from pyspark.sql.functions import udf, mean, lit, stddev, col, expr, when\n",
    "\n",
    "sample_rate = 8000\n",
    "\n",
    "def init():\n",
    "    from pyspark.ml import PipelineModel\n",
    "    global pipeline\n",
    "    dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "    pipeline = PipelineModel.load(os.path.join(dir_path, 'model'))\n",
    "\n",
    "def run(input_df):\n",
    "    def extract_amplitude_spectrum():\n",
    "        def m(x):\n",
    "            ampl = np.abs(np.fft.fft(x)) / sample_rate\n",
    "            return ampl[:sample_rate // 2 + 1].tolist()\n",
    "\n",
    "        return udf(m, ArrayType(DoubleType()))\n",
    "\n",
    "    def extract_dominant_frequencies(index):\n",
    "        def m(ff):\n",
    "            freq = np.fft.fftfreq(sample_rate, d = 1/sample_rate)[:sample_rate // 2 + 1]\n",
    "            return [float(f[index]) for f in sorted(list(zip(freq, ff)), key = lambda x: x[1], reverse = True)]\n",
    "    \n",
    "        return udf(m, ArrayType(DoubleType()))\n",
    "    \n",
    "    input_df = input_df.withColumn(\"fft\", extract_amplitude_spectrum()(input_df.vibration))\n",
    "    dfa = (input_df\n",
    "       .withColumn(\"dominant_frequencies\", extract_dominant_frequencies(0)(input_df.fft))\n",
    "       .withColumn(\"dominant_frequencies_amplitudes\", extract_dominant_frequencies(1)(input_df.fft)))\n",
    "    \n",
    "    frequency_features = 3\n",
    "    dfa = reduce(lambda _df, i: _df.withColumn('f{0}'.format(i), _df.dominant_frequencies[i]), range(frequency_features), dfa)\n",
    "    dfa = reduce(lambda _df, i: _df.withColumn('a{0}'.format(i), _df.dominant_frequencies_amplitudes[i]), range(frequency_features), dfa)\n",
    "    dfa = dfa.drop('vibration', 'fft', 'dominant_frequencies', 'dominant_frequencies_amplitudes', 'ambient_pressure', 'ambient_temperature')\n",
    "    \n",
    "    dependent_features = [c for c in dfa.columns if c not in ['machineID', 'EnqueuedTimeUtc', 'speed']]\n",
    "    dfa = reduce(lambda _df, f: _df.withColumn('{0}_n'.format(f), col(f) / col('speed')), dependent_features, dfa)\n",
    "    \n",
    "    features = [c for c in dfa.columns if c not in ['machineID', 'EnqueuedTimeUtc']]\n",
    "\n",
    "    # assemble features\n",
    "    va = VectorAssembler(inputCols=(features), outputCol='features')\n",
    "\n",
    "    feat_data = va.transform(dfa)\n",
    "\n",
    "    predictions = pipeline.transform(feat_data).collect()\n",
    "\n",
    "    #Get each scored result\n",
    "    preds = [str(x['prediction']) for x in predictions]\n",
    "    return \",\".join(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../score.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    #sc.stop()\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    init()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0,3.0,3.0'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
