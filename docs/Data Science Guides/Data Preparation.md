First, let us focus on the data requirements to build a successful predictive maintenance solution.

**Data Relevancy** A predictive model is "trained" to recognize hidden patterns from historical data on the equipment. Typically, this data has descriptive features or attributes of a piece of equipment, and a specific "target" behavior of the equipment for those specific set of features. In the simplest case, the training data is a sufficient set of examples where the target value is a  true or false - indicating whether an equipment failed under specific conditions of temperature, pressure, vibration, torque, and more such attributes that impact the equipment's behavior. A model trained with such examples is then expected to "predict" the target value when provided just the features of new examples. To make accurate predictions, it is important for the model to accurately capture the relationship between features and the target of prediction. So a key prerequisite is to have *representative* training data, which have features that have "predictive power" towards the target of prediction; meaning the data should be relevant to the prediction goal to expect accurate predictions. Examples:

- If the target is to predict failures of turbines, the training attributes should contain telemetry reflecting the status of the turbine blades and the rotor and other components that make up the turbine under varying conditions of inputs.
- If the target is to predict failure of an electronic components, measurements of voltage, current, resistance, capacitance besides heat, dust level, and other factors need to be recorded in terms of the relevance to the actual failure.

The data also needs to be directly related to the operating conditions of the target being predicted, at the appropriate granularity. In the turbine example, the target of failure prediction could be the entire turbine, or a specific component of the turbine, such as the blade, the shaft, the yaw drive, gear or the motor; or even their subcomponents.  of these individual components. It would be incorrect to build a failure model for the more general component without the data from all sub-components; conversely, predicting the failure of a subcomponent based on higher level data would also be inaccurate. In general, it is more sensible to predict specific failure events than more general ones.

To build predictive models, the user should understand the data relevancy requirements and provide the domain knowledge required select relevant subsets of data for analysis. Three essential data sources are required to build accurate models for predictive maintenance solutions:

- *Failure History:* When building predictive models that predict failures, the ML algorithm needs to learn the normal operation pattern as well as the failure pattern through the training process. It is essential for the training data to contain sufficient number of examples in both categories in order to learn these two different patterns. Failure events can be found in maintenance records and parts replacement history or anomalies in the training data can also be used as failures as identified by the domain experts. But in typical scenarios, failure events are rare. There are however [some advanced techniques](https://blogs.technet.microsoft.com/machinelearning/2016/04/19/evaluating-failure-prediction-models-for-predictive-maintenance/) to handle this data imbalance.
- *Maintenance/Repair History:* An essential source of data for predictive maintenance solutions is the detailed maintenance history of the asset containing information about the components replaced, preventive maintenance activates performed, etc. It is extremely important to capture these events as they record the degradation patterns.
- *Machine Conditions:* In order to predict how many more days (hours, miles, transactions, etc.) a machine lasts before it fails, we assume the machineâ€™s health status degrades over time during its operation. Therefore, we expect the data to contain time-varying features that capture this ageing pattern and any anomalies that lead to degradation. In IoT applications, the telemetry data from different sensors represent one good example. In order to predict if a machine is going to fail within a time frame, ideally the data should capture degrading trend during this time frame before the actual failure event.

**Data Sufficiency** A typical question is how many failure events are required to train a model, and how many is considered as "enough"? The answer to this question is still "it depends" - but there are well known heuristics and iterative methodologies to determine data sufficiency - please refer the discussions [here](https://machinelearningmastery.com/much-training-data-required-machine-learning/).

https://datascience.stackexchange.com/questions/19980/how-much-data-are-sufficient-to-train-my-machine-learning-model

**END**
